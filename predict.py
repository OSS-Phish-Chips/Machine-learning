# app.py
from fastapi import FastAPI, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import joblib
from urllib.parse import urlparse
import re
import math

app = FastAPI()

# CORS (í™•ì¥/ë¡œì»¬ì—ì„œ ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í•„ìš”ì‹œ ì¢í˜€ë„ ë¨
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# âœ”ï¸ í•„ìš” ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
import re
import socket
import time
import urllib.request
import requests
import warnings
import whois
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from googlesearch import search
from patterns import *
import urllib3
import random
from urllib.parse import urlparse

from tqdm import tqdm
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings("ignore")

# Path of your local server. Different for different OSs.
LOCALHOST_PATH = "/Library/WebServer/Documents/"
DIRECTORY_NAME = "Malicious-Web-Content-Detection-Using-Machine-Learning"

# ============================================================================
# ğŸ”§ ê°œì„ ëœ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================================

# ===== Patch: robust DNS check (IPv4/IPv6) =====
def robust_dns_check(hostname: str) -> bool:
    try:
        # IPv4/IPv6 ëª¨ë‘ ì¡°íšŒ
        infos = socket.getaddrinfo(hostname, None)
        return len(infos) > 0
    except socket.gaierror:
        # 'www.' ì œê±° ì¬ì‹œë„
        if hostname.startswith('www.'):
            try:
                return len(socket.getaddrinfo(hostname[4:], None)) > 0
            except socket.gaierror:
                return False
        return False

def safe_url_request_improved(url, max_retries=2, timeout=10):
    """ê°œì„ ëœ ì•ˆì „í•œ URL ìš”ì²­ ì²˜ë¦¬"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
    }
    
    # URL ì „ì²˜ë¦¬
    if not url.startswith(('http://', 'https://')):
        url = 'http://' + url
    
    for attempt in range(max_retries + 1):
        try:
            if attempt > 0:
                time.sleep(random.uniform(0.5, 2))
            
            response = requests.get(
                url, 
                headers=headers,
                timeout=timeout,
                verify=False,
                allow_redirects=True,
                stream=True
            )
            
            response.raise_for_status()
            
            # ì½˜í…ì¸  í¬ê¸° ì œí•œ
            content_length = response.headers.get('content-length')
            if content_length and int(content_length) > 5 * 1024 * 1024:  # 5MB ì œí•œ
                return None, None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            return response, soup
            
        except (requests.exceptions.SSLError, 
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                requests.exceptions.TooManyRedirects) as e:
            if attempt == max_retries:
                return None, None
            continue
        except Exception:
            return None, None
    
    return None, None

# ===== Patch: whois (ë‘ íŒ¨í‚¤ì§€ ëª¨ë‘ í˜¸í™˜) =====
def safe_whois_query(hostname):
    """
    - pip install whois  (whois.query)
    - ë˜ëŠ” pip install python-whois (whois.whois)
    ë‘˜ ì¤‘ ë¬´ì—‡ì´ ê¹”ë ¤ ìˆì–´ë„ ë™ì‘í•˜ë„ë¡ ì²˜ë¦¬.
    """
    try:
        # ì‹œë‚˜ë¦¬ì˜¤ 1: whois.query API
        q = getattr(whois, 'query', None)
        if callable(q):
            domain = q(hostname)
            if domain:
                return domain
    except Exception:
        pass

    try:
        # ì‹œë‚˜ë¦¬ì˜¤ 2: whois.whois API (dict-like)
        w = getattr(whois, 'whois', None)
        if callable(w):
            data = w(hostname)
            class _Domain:
                def __init__(self, d):
                    self.name = d.get('domain_name')
                    self.creation_date = d.get('creation_date')
                    self.expiration_date = d.get('expiration_date')
            return _Domain(data) if data else None
    except Exception:
        pass

    # 'www.' ì œê±° ì¬ì‹œë„
    if hostname.startswith('www.'):
        return safe_whois_query(hostname[4:])

    return None

# ============================================================================
# ğŸ¯ ê¸°ì¡´ íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜ë“¤ (ê·¸ëŒ€ë¡œ ìœ ì§€)
# ============================================================================

def having_ip_address(url):
    ipv4_pattern = r'(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5]))'
    ipv6_pattern = r'(([0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,7}:|([0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|([0-9a-fA-F]{1,4}:){1,5}(:[0-9a-fA-F]{1,4}){1,2}|([0-9a-fA-F]{1,4}:){1,4}(:[0-9a-fA-F]{1,4}){1,3}|([0-9a-fA-F]{1,4}:){1,3}(:[0-9a-fA-F]{1,4}){1,4}|([0-9a-fA-F]{1,4}:){1,2}(:[0-9a-fA-F]{1,4}){1,5}|[0-9a-fA-F]{1,4}:((:[0-9a-fA-F]{1,4}){1,6})|:((:[0-9a-fA-F]{1,4}){1,7}|:)|fe80:(:[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|::(ffff(:0{1,4}){0,1}:){0,1}((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])|([0-9a-fA-F]{1,4}:){1,4}:((25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}(25[0-5]|(2[0-4]|1{0,1}[0-9]){0,1}[0-9]))'
    ip_address_pattern = ipv4_pattern + "|" + ipv6_pattern
    match = re.search(ip_address_pattern, url)
    return -1 if match else 1

def url_length(url):
    if len(url) < 54:
        return 1
    if 54 <= len(url) <= 75:
        return 0
    return -1

def shortening_service(url):
    shortening_services = r"bit\.ly|goo\.gl|shorte\.st|go2l\.ink|x\.co|ow\.ly|t\.co|tinyurl|tr\.im|is\.gd|cli\.gs|" \
                         r"yfrog\.com|migre\.me|ff\.im|tiny\.cc|url4\.eu|twit\.ac|su\.pr|twurl\.nl|snipurl\.com|" \
                         r"short\.to|BudURL\.com|ping\.fm|post\.ly|Just\.as|bkite\.com|snipr\.com|fic\.kr|loopt\.us|" \
                         r"doiop\.com|short\.ie|kl\.am|wp\.me|rubyurl\.com|om\.ly|to\.ly|bit\.do|t\.co|lnkd\.in|" \
                         r"db\.tt|qr\.ae|adf\.ly|goo\.gl|bitly\.com|cur\.lv|tinyurl\.com|ow\.ly|bit\.ly|ity\.im|" \
                         r"q\.gs|is\.gd|po\.st|bc\.vc|twitthis\.com|u\.to|j\.mp|buzurl\.com|cutt\.us|u\.bb|yourls\.org|" \
                         r"x\.co|prettylinkpro\.com|scrnch\.me|filoops\.info|vzturl\.com|qr\.net|1url\.com|tweez\.me|v\.gd|" \
                         r"tr\.im|link\.zip\.net"
    match = re.search(shortening_services, url)
    return -1 if match else 1

def having_at_symbol(url):
    match = re.search('@', url)
    return -1 if match else 1

def double_slash_redirecting(url):
    last_double_slash = url.rfind('//')
    return -1 if last_double_slash > 6 else 1

def prefix_suffix(domain):
    match = re.search('-', domain)
    return -1 if match else 1

def having_sub_domain(url):
    if having_ip_address(url) == -1:
        match = re.search(
            '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'
            '([01]?\\d\\d?|2[0-4]\\d|25[0-5]))|(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',
            url)
        pos = match.end()
        url = url[pos:]
    num_dots = [x.start() for x in re.finditer(r'\.', url)]
    if len(num_dots) <= 3:
        return 1
    elif len(num_dots) == 4:
        return 0
    else:
        return -1

def domain_registration_length(domain):
    try:
        expiration_date = domain.expiration_date
        today = datetime.now()
        
        if expiration_date:
            if isinstance(expiration_date, list):
                expiration_date = expiration_date[0]
            registration_length = abs((expiration_date - today).days)
            return 1 if registration_length / 365 > 1 else -1
    except:
        pass
    return 0  # ì¤‘ë¦½ê°’


# ===== Patch: favicon (ì¡°ê¸° ë°˜í™˜ ì œê±°, ì •í™•ë„ ê°œì„ ) =====
def favicon(wiki, soup, domain):
    if not soup:
        return 0
    try:
        links = soup.find_all('link', href=True)
        if not links:
            return 1
        safe_cnt, total = 0, 0
        for link in links:
            href = link.get('href', '').strip()
            if not href:
                continue
            total += 1
            # ì ˆëŒ€/ìƒëŒ€ URL normalize
            if href.startswith('//'):
                href_host = get_hostname_from_url('http:' + href)
            elif href.startswith(('http://', 'https://')):
                href_host = get_hostname_from_url(href)
            else:
                href_host = domain  # ìƒëŒ€ê²½ë¡œë©´ same-origin ì·¨ê¸‰
            if domain in href_host or (wiki and get_hostname_from_url(wiki) in href):
                safe_cnt += 1
        if total == 0:
            return 1
        ratio = safe_cnt / total * 100
        return 1 if ratio >= 50 else (0 if ratio >= 20 else -1)
    except Exception:
        return 1

def https_token(url):
    http_https = r'https://|http://'
    match = re.search(http_https, url)
    if match and match.start() == 0:
        url = url[match.end():]
    match = re.search('http|https', url)
    return -1 if match else 1

def request_url(wiki, soup, domain):
    if not soup:
        return 0
    try:
        i = 0
        success = 0
        
        for tag_name in ['img', 'audio', 'embed', 'iframe']:
            for tag in soup.find_all(tag_name, src=True):
                src = tag.get('src', '')
                dots = [x.start() for x in re.finditer(r'\.', src)]
                if wiki in src or domain in src or len(dots) == 1:
                    success += 1
                i += 1
        
        if i == 0:
            return 1
            
        percentage = success / float(i) * 100
        if percentage < 22.0:
            return 1
        elif 22.0 <= percentage < 61.0:
            return 0
        else:
            return -1
    except:
        return 1

def url_of_anchor(wiki, soup, domain):
    if not soup:
        return 0
    try:
        i = 0
        unsafe = 0
        for a in soup.find_all('a', href=True):
            href = a.get('href', '')
            if "#" in href or "javascript" in href.lower() or "mailto" in href.lower() or not (
                    wiki in href or domain in href):
                unsafe += 1
            i += 1
        
        if i == 0:
            return 1
            
        percentage = unsafe / float(i) * 100
        if percentage < 31.0:
            return 1
        elif 31.0 <= percentage < 67.0:
            return 0
        else:
            return -1
    except:
        return 1

def links_in_tags(wiki, soup, domain):
    if not soup:
        return 0
    try:
        i = 0
        success = 0
        
        for link in soup.find_all('link', href=True):
            href = link.get('href', '')
            dots = [x.start() for x in re.finditer(r'\.', href)]
            if wiki in href or domain in href or len(dots) == 1:
                success += 1
            i += 1

        for script in soup.find_all('script', src=True):
            src = script.get('src', '')
            dots = [x.start() for x in re.finditer(r'\.', src)]
            if wiki in src or domain in src or len(dots) == 1:
                success += 1
            i += 1
        
        if i == 0:
            return 1
            
        percentage = success / float(i) * 100
        if percentage < 17.0:
            return 1
        elif 17.0 <= percentage < 81.0:
            return 0
        else:
            return -1
    except:
        return 1

# ===== Patch: sfh (ì—¬ëŸ¬ form ì¢…í•© íŒë‹¨) =====
def sfh(wiki, soup, domain):
    if not soup:
        return 1
    try:
        forms = soup.find_all('form', action=True)
        if not forms:
            return 1
        neg, mid, pos = 0, 0, 0
        for form in forms:
            action = (form.get('action') or '').strip().lower()
            if action in ("", "about:blank"):
                neg += 1
            elif (domain and domain.lower() in action) or (wiki and get_hostname_from_url(wiki).lower() in action):
                pos += 1
            else:
                mid += 1
        # ë‹¤ìˆ˜ê²°
        if neg > max(mid, pos):
            return -1
        if mid >= pos:
            return 0
        return 1
    except Exception:
        return 1

def submitting_to_email(soup):
    if not soup:
        return 1
    try:
        for form in soup.find_all('form', action=True):
            action = form.get('action', '')
            if "mailto:" in action:
                return -1
    except:
        pass
    return 1

def abnormal_url(domain, url):
    try:
        if domain and hasattr(domain, 'name') and domain.name:
            hostname = domain.name
            match = re.search(hostname, url)
            return 1 if match else -1
    except:
        pass
    return 0

def i_frame(soup):
    if not soup:
        return 1
    try:
        for iframe in soup.find_all(['iframe', 'i_frame'], width=True, height=True):
            width = iframe.get('width', '')
            height = iframe.get('height', '')
            frameborder = iframe.get('frameBorder', iframe.get('frameborder', ''))
            
            if width == "0" and height == "0" and frameborder == "0":
                return -1
            if width == "0" or height == "0" or frameborder == "0":
                return 0
    except:
        pass
    return 1

def age_of_domain(domain):
    try:
        if domain and hasattr(domain, 'creation_date') and domain.creation_date:
            creation_date = domain.creation_date
            if isinstance(creation_date, list):
                creation_date = creation_date[0]
            
            today = datetime.now()
            age_days = abs((today - creation_date).days)
            return 1 if age_days / 30 >= 6 else -1
    except:
        pass
    return 0

def web_traffic_alternative(url):
    """Alexa ëŒ€ì‹  ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ íŠ¸ë˜í”½ ì¶”ì •"""
    try:
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ë„ë©”ì¸ ê¸¸ì´ì™€ êµ¬ì¡°ë¡œ íŒë‹¨
        hostname = get_hostname_from_url(url)
        
        # ìœ ëª…í•œ ë„ë©”ì¸ë“¤ (ê°„ë‹¨í•œ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸)
        popular_domains = [
            'google.com', 'youtube.com', 'facebook.com', 'twitter.com', 
            'instagram.com', 'linkedin.com', 'github.com', 'stackoverflow.com',
            'wikipedia.org', 'amazon.com', 'microsoft.com', 'apple.com'
        ]
        
        for domain in popular_domains:
            if domain in hostname.lower():
                return 1
        
        # ë„ë©”ì¸ êµ¬ì¡° ê¸°ë°˜ ì ìˆ˜
        if len(hostname.split('.')) <= 2 and len(hostname) < 15:
            return 1
        elif len(hostname) > 30 or len(hostname.split('.')) > 4:
            return -1
        else:
            return 0
    except:
        return 0

# ===== Patch: google index (íŒ¨í‚¤ì§€ ì°¨ì´ í¡ìˆ˜ + ë¹ ë¥¸ ì‹¤íŒ¨) =====
def google_index_safe(url):
    """
    googlesearch íŒ¨í‚¤ì§€ êµ°ì˜ íŒŒë¼ë¯¸í„° ì°¨ì´ë¥¼ í¡ìˆ˜.
    ì‹¤íŒ¨/ì°¨ë‹¨ ì‹œ 0 ë°˜í™˜.
    """
    try:
        time.sleep(random.uniform(0.3, 0.8))  # rate-limit ë°°ë ¤(ì§§ê²Œ)
        query = f"site:{get_hostname_from_url(url)}"
        # ìš°ì„  ê¸°ë³¸ ì‹œê·¸ë‹ˆì²˜
        try:
            results = list(search(query, num_results=3))
            return 1 if results else -1
        except TypeError:
            # ëŒ€ì²´ ì‹œê·¸ë‹ˆì²˜ (ë‹¤ë¥¸ íŒ¨í‚¤ì§€)
            results = list(search(query, num=3, stop=3, pause=0.5))
            return 1 if results else -1
    except Exception:
        return 0

def statistical_report(url, hostname):
    """í†µê³„ ë¦¬í¬íŠ¸ (IP ê¸°ë°˜)"""
    try:
        ip_address = socket.gethostbyname(hostname)
    except:
        return 0  # DNS ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ê°’
    
    # ì•Œë ¤ì§„ ì•…ì„± ë„ë©”ì¸/IP íŒ¨í„´
    url_match = re.search(
        r'at\.ua|usa\.cc|baltazarpresentes\.com\.br|pe\.hu|esy\.es|hol\.es|sweddy\.com|myjino\.ru|96\.lt|ow\.ly', url)
    ip_match = re.search(
        '146\.112\.61\.108|213\.174\.157\.151|121\.50\.168\.88|192\.185\.217\.116|78\.46\.211\.158|181\.174\.165\.13|46\.242\.145\.103|121\.50\.168\.40|83\.125\.22\.219|46\.242\.145\.98|'
        '107\.151\.148\.44|107\.151\.148\.107|64\.70\.19\.203|199\.184\.144\.27|107\.151\.148\.108|107\.151\.148\.109|119\.28\.52\.61|54\.83\.43\.69|52\.69\.166\.231|216\.58\.192\.225|'
        '118\.184\.25\.86|67\.208\.74\.71|23\.253\.126\.58|104\.239\.157\.210|175\.126\.123\.219|141\.8\.224\.221|10\.10\.10\.10|43\.229\.108\.32|103\.232\.215\.140|69\.172\.201\.153|'
        '216\.218\.185\.162|54\.225\.104\.146|103\.243\.24\.98|199\.59\.243\.120|31\.170\.160\.61|213\.19\.128\.77|62\.113\.226\.131|208\.100\.26\.234|195\.16\.127\.102|195\.16\.127\.157|'
        '34\.196\.13\.28|103\.224\.212\.222|172\.217\.4\.225|54\.72\.9\.51|192\.64\.147\.141|198\.200\.56\.183|23\.253\.164\.103|52\.48\.191\.26|52\.214\.197\.72|87\.98\.255\.18|209\.99\.17\.27|'
        '216\.38\.62\.18|104\.130\.124\.96|47\.89\.58\.141|78\.46\.211\.158|54\.86\.225\.156|54\.82\.156\.19|37\.157\.192\.102|204\.11\.56\.48|110\.34\.231\.42',
        ip_address)
    
    if url_match or ip_match:
        return -1
    else:
        return 1

def get_hostname_from_url(url: str) -> str:
    if not url.startswith(('http://', 'https://')):
        url = 'http://' + url  # scheme ë³´ì •
    parsed = urlparse(url)
    host = parsed.netloc or parsed.path.split('/')[0]
    # strip 'www.'
    if host.lower().startswith('www.'):
        host = host[4:]
    return host

# ============================================================================
# ğŸš€ ê°œì„ ëœ ë©”ì¸ í•¨ìˆ˜ (ë‹¨ê³„ë³„ fallback ì ìš©)
# ============================================================================

def extract_features_with_fallback(url):
    """
    ë‹¨ê³„ë³„ fallbackì´ ì ìš©ëœ íŠ¹ì§• ì¶”ì¶œ í•¨ìˆ˜
    """
    status = []
    hostname = get_hostname_from_url(url)
    
    # === 1ë‹¨ê³„: URL ê¸°ë°˜ íŠ¹ì§• (í•­ìƒ ì¶”ì¶œ ê°€ëŠ¥) ===
    try:
        status.extend([
            having_ip_address(url),          # F1
            url_length(url),                 # F2  
            shortening_service(url),         # F3
            having_at_symbol(url),           # F4
            double_slash_redirecting(url),   # F5
            prefix_suffix(hostname),         # F6
            having_sub_domain(url),          # F7
        ])
    except Exception as e:
        print(f"URL íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        status.extend([0] * 7)
    
    # === 2ë‹¨ê³„: DNS í•´ì„ ë° whois ì¡°íšŒ ===
    dns_available = robust_dns_check(hostname)
    domain = None
    
    if dns_available:
        domain = safe_whois_query(hostname)
        dns_status = 1 if domain else 0
    else:
        dns_status = -1
    
    # ë„ë©”ì¸ ë“±ë¡ ê¸°ê°„
    try:
        if domain:
            status.append(domain_registration_length(domain))  # F8
        else:
            status.append(0 if dns_available else -1)
    except:
        status.append(0)
    
    # === 3ë‹¨ê³„: HTTP ìš”ì²­ ë° HTML íŒŒì‹± ===
    if dns_available:
        response, soup = safe_url_request_improved(url)
        http_available = (response is not None and soup is not None)
    else:
        response, soup = None, None
        http_available = False
    
    # HTML ê¸°ë°˜ íŠ¹ì§•ë“¤
    try:
        if http_available:
            status.extend([
                favicon(url, soup, hostname),           # F9
                https_token(url),                       # F10
                request_url(url, soup, hostname),       # F11
                url_of_anchor(url, soup, hostname),     # F12
                links_in_tags(url, soup, hostname),     # F13
                sfh(url, soup, hostname),               # F14
                submitting_to_email(soup),              # F15
            ])
        else:
            # HTTP ì‹¤íŒ¨ ì‹œ URL ê¸°ë°˜ìœ¼ë¡œë§Œ íŒë‹¨ ê°€ëŠ¥í•œ ê²ƒë“¤
            status.extend([
                0,                           # F9 - favicon (ì¤‘ë¦½)
                https_token(url),           # F10 - https_token (URLë§Œìœ¼ë¡œ íŒë‹¨ ê°€ëŠ¥)
                0,                          # F11 - request_url (ì¤‘ë¦½)
                0,                          # F12 - url_of_anchor (ì¤‘ë¦½)
                0,                          # F13 - links_in_tags (ì¤‘ë¦½)
                1,                          # F14 - sfh (ê¸°ë³¸ê°’: ì•ˆì „)
                1,                          # F15 - submitting_to_email (ê¸°ë³¸ê°’: ì•ˆì „)
            ])
    except Exception as e:
        print(f"HTML íŠ¹ì§• ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        status.extend([0] * 7)
    
    # === 4ë‹¨ê³„: ë„ë©”ì¸ ê¸°ë°˜ íŠ¹ì§•ë“¤ ===
    try:
        if domain:
            status.extend([
                abnormal_url(domain, url),    # F16
                i_frame(soup),                # F17
                age_of_domain(domain),        # F18
            ])
        else:
            status.extend([
                0 if dns_available else -1,  # F16
                i_frame(soup) if http_available else 1,  # F17
                0 if dns_available else -1,  # F18
            ])
    except:
        status.extend([0, 1, 0])
    
    # === 5ë‹¨ê³„: ì™¸ë¶€ ì„œë¹„ìŠ¤ ì˜ì¡´ íŠ¹ì§•ë“¤ ===
    status.append(dns_status)                    # F19 - DNS
    status.append(web_traffic_alternative(url))  # F20 - ì›¹ íŠ¸ë˜í”½ (ëŒ€ì²´ ë°©ë²•)
    
    # Google ì¸ë±ìŠ¤ (rate limiting ê³ ë ¤)
    try:
        status.append(google_index_safe(url))    # F21
    except:
        status.append(0)
    
    # í†µê³„ ë¦¬í¬íŠ¸
    try:
        status.append(statistical_report(url, hostname))  # F22
    except:
        status.append(0)
    
    return status

# ============================================================================
# ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ (ê°œì„ ë¨)
# ============================================================================

def extract_features_batch_robust(urls, batch_size=25):
    """
    ê²¬ê³ í•œ ë°°ì¹˜ ì²˜ë¦¬ - ë” ì‘ì€ ë°°ì¹˜ì™€ ë” ë§ì€ íœ´ì‹
    """
    features_list = []
    failed_urls = []
    
    for i in range(0, len(urls), batch_size):
        batch_urls = urls[i:i+batch_size]
        batch_num = i//batch_size + 1
        total_batches = (len(urls)-1)//batch_size + 1
        
        print(f"ğŸ“¦ ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘ ({len(batch_urls)}ê°œ URL)...")
        
        batch_features = []
        for j, url in enumerate(tqdm(batch_urls, desc=f"ë°°ì¹˜ {batch_num}")):
            try:
                features = extract_features_with_fallback(url)
                batch_features.append(features)
                
                # URL ê°„ íœ´ì‹ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                if j < len(batch_urls) - 1:
                    time.sleep(random.uniform(0.1, 0.5))
                    
            except Exception as e:
                print(f"ğŸ’¥ URL ì²˜ë¦¬ ì‹¤íŒ¨: {url[:50]}... - {str(e)[:100]}")
                batch_features.append([0] * 22)  # ì¤‘ë¦½ê°’ìœ¼ë¡œ ì„¤ì •
                failed_urls.append(url)
        
        features_list.extend(batch_features)
        
        # ë°°ì¹˜ ê°„ íœ´ì‹
        if i + batch_size < len(urls):
            sleep_time = random.uniform(2, 5)
            print(f"ğŸ’¤ {sleep_time:.1f}ì´ˆ íœ´ì‹...")
            time.sleep(sleep_time)
    
    print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ! ì‹¤íŒ¨í•œ URL: {len(failed_urls)}ê°œ")
    if failed_urls:
        print("ì‹¤íŒ¨ URL ìƒ˜í”Œ:", failed_urls[:5])
    
    return features_list

# ============================================================================
# ğŸ“Š CSV ì²˜ë¦¬ í•¨ìˆ˜ (ê·¸ëŒ€ë¡œ ìœ ì§€)
# ============================================================================

def extract_features_from_csv(csv_path, url_column, label_column=None):
    """CSVì—ì„œ URL ë¡œë“œ + íŠ¹ì§• ì¶”ì¶œ"""
    df = pd.read_csv(csv_path, encoding="latin1") 
    features_list = []
    labels = []

    print(f"ğŸ“„ CSV ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ URL")
    
    for _, row in tqdm(df.iterrows(), total=len(df), desc="íŠ¹ì§• ì¶”ì¶œ"):
        url = row[url_column]
        try:
            features = extract_features_with_fallback(url)
        except Exception as e:
            print(f"í–‰ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            features = [0] * 22
        features_list.append(features)

        if label_column:
            labels.append(row[label_column])

    feature_df = pd.DataFrame(features_list, columns=[f"F{i}" for i in range(1, 23)])

    if label_column:
        feature_df["label"] = labels

    return feature_df


# 1) ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "random_forest_model_5000.pkl"
model = joblib.load(MODEL_PATH)

# 2) ì™„ì„±ëœ extract_features í•¨ìˆ˜
def extract_features(url: str):
    """
    22ê°œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ - ê¸°ì¡´ extract_features_with_fallbackì„ ì‚¬ìš©
    """
    return extract_features_with_fallback(url)

class PredictResponse(BaseModel):
    label: int
    proba: float
    features: dict

@app.get("/predict", response_model=PredictResponse)
def predict(url: str = Query(..., description="http(s)://ë¡œ ì‹œì‘í•˜ëŠ” URL")):
    feats = extract_features(url)
    
    # 22ê°œ íŠ¹ì§•ì´ ëª¨ë‘ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
    if len(feats) != 22:
        feats = feats + [0] * (22 - len(feats))  # ë¶€ì¡±í•œ íŠ¹ì§•ì€ 0ìœ¼ë¡œ ì±„ì›€
    
    # ëª¨ë¸ ì…ë ¥ ì°¨ì›ì— ë§ì¶”ê¸°
    X = [feats]
    
    # ë¶„ë¥˜/í™•ë¥ 
    try:
        proba = float(model.predict_proba(X)[0][1])
    except Exception:
        # ëª¨ë¸ì´ predict_proba ì—†ìœ¼ë©´ ì˜ˆì™¸ â†’ ëŒ€ì‹  predictë§Œ
        proba = float(model.predict(X)[0])

    label = 1 if proba >= 0.5 else 0
    
    # íŠ¹ì§•ë“¤ì„ ì˜ë¯¸ìˆëŠ” ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
    feature_names = [
        "having_ip_address", "url_length", "shortening_service", "having_at_symbol",
        "double_slash_redirecting", "prefix_suffix", "having_sub_domain", 
        "domain_registration_length", "favicon", "https_token", "request_url",
        "url_of_anchor", "links_in_tags", "sfh", "submitting_to_email",
        "abnormal_url", "i_frame", "age_of_domain", "dns_record", "web_traffic",
        "google_index", "statistical_report"
    ]
    
    features_dict = {name: feats[i] for i, name in enumerate(feature_names)}
    
    return PredictResponse(
        label=label,
        proba=proba,
        features=features_dict
    )

# ì¶”ê°€ ì—”ë“œí¬ì¸íŠ¸: ë°°ì¹˜ ì˜ˆì¸¡
@app.post("/predict_batch")
def predict_batch(urls: list[str]):
    """ì—¬ëŸ¬ URLì— ëŒ€í•œ ë°°ì¹˜ ì˜ˆì¸¡"""
    results = []
    
    for url in urls:
        try:
            feats = extract_features(url)
            if len(feats) != 22:
                feats = feats + [0] * (22 - len(feats))
            
            X = [feats]
            try:
                proba = float(model.predict_proba(X)[0][1])
            except:
                proba = float(model.predict(X)[0])
            
            label = 1 if proba >= 0.5 else 0
            
            results.append({
                "url": url,
                "label": label,
                "proba": proba
            })
        except Exception as e:
            results.append({
                "url": url,
                "label": -1,  # ì˜¤ë¥˜ í‘œì‹œ
                "proba": 0.0,
                "error": str(e)
            })
    
    return {"results": results}

# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
@app.get("/health")
def health_check():
    return {"status": "healthy", "model_loaded": True}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
